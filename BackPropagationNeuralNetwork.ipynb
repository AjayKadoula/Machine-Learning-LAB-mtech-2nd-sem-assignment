{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " BackPropagationNeuralNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVoChQh/Kdri2yqViLc5t/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjayKadoula/Mtech_Lab_1/blob/main/BackPropagationNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GStXa5ia0Hf3"
      },
      "source": [
        "# **Back Propagation Neural Network**\r\n",
        "\r\n",
        "Backpropagation is an algorithm how to compute gradient of cost function. To simply put this, back-propagation is nothing but similar to how humans learn from their mistakes. Let suppose you are practicing soccer shots, you want to hit the goal post, the very first time you strike the ball, you miss the aim which you decided, then you analyze that okay, I have to adjust the hitting pace or the angle based on the previous shot. This is basically the error which you computed. Human brains are smart enough to coordinate the mind and the body part which is used in a particular activity and that is how you change your way of doing the things. Similarly, in neural networks, back-propagation is the way how you teach a neural network that by this angle or by this pace, you missed the aim, next time be more accurate.\r\n",
        "\r\n",
        "If you again miss the shot, you will again try to improvise the way you hit last time. This process will continue till you hit the right shot. So, this is most analogous example that I found to be relevant.\r\n",
        "\r\n",
        "This is the theory behind back-propagation. There is a lot of mathematics involved in this. These are the things on the top of my head right now which you should know to learn the back-propagation:\r\n",
        "\r\n",
        "\r\n",
        "1. Partial Differentiation\r\n",
        "2. Linear Algebra\r\n",
        "3. Concept of weights and bias\r\n",
        "4. Hidden Layers\r\n",
        "5. Neurons\r\n",
        "6. Vectors\r\n",
        "7. Input Dimensions\r\n",
        "8. Output Dimensions\r\n",
        "9. Concept of learning rate\r\n",
        "\r\n",
        "Backpropagation is just a special name given to finding the gradient of the cost function in a neural network. There's really no magic going on, just some reasonably straight forward calculus.\r\n",
        "\r\n",
        "In deep learning back propagation means transmission\r\n",
        "of information, and that information relates to the\r\n",
        "error produced by the neural network when it makes\r\n",
        "a guess about data. Back propagation takes the error\r\n",
        "associated with a wrong guess by a neural network,\r\n",
        "and uses that error to adjust the neural networkâ€™s\r\n",
        "parameters in the direction of less error.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OM9YJyI3SOe"
      },
      "source": [
        "# **Implementation : -**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQpV2tZvzvI8",
        "outputId": "5e2929a0-6832-48a2-f433-86c095b21164"
      },
      "source": [
        "import math\r\n",
        "import random\r\n",
        "import string\r\n",
        "\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "# calculate a random number where:  a <= rand < b\r\n",
        "def rand(a, b):\r\n",
        "    return (b-a)*random.random() + a\r\n",
        "\r\n",
        "# Make a matrix (we could use NumPy to speed this up)\r\n",
        "def makeMatrix(I, J, fill=0.0):\r\n",
        "    m = []\r\n",
        "    for i in range(I):\r\n",
        "        m.append([fill]*J)\r\n",
        "    return m\r\n",
        "\r\n",
        "# our sigmoid function, tanh is a little nicer than the standard 1/(1+e^-x)\r\n",
        "def sigmoid(x):\r\n",
        "    return math.tanh(x)\r\n",
        "\r\n",
        "# derivative of our sigmoid function, in terms of the output (i.e. y)\r\n",
        "def dsigmoid(y):\r\n",
        "    return 1.0 - y**2\r\n",
        "\r\n",
        "class NN:\r\n",
        "    def __init__(self, ni, nh, no):\r\n",
        "        # number of input, hidden, and output nodes\r\n",
        "        self.ni = ni + 1 # +1 for bias node\r\n",
        "        self.nh = nh\r\n",
        "        self.no = no\r\n",
        "\r\n",
        "        # activations for nodes\r\n",
        "        self.ai = [1.0]*self.ni\r\n",
        "        self.ah = [1.0]*self.nh\r\n",
        "        self.ao = [1.0]*self.no\r\n",
        "        \r\n",
        "        # create weights\r\n",
        "        self.wi = makeMatrix(self.ni, self.nh)\r\n",
        "        self.wo = makeMatrix(self.nh, self.no)\r\n",
        "        # set them to random vaules\r\n",
        "        for i in range(self.ni):\r\n",
        "            for j in range(self.nh):\r\n",
        "                self.wi[i][j] = rand(-0.2, 0.2)\r\n",
        "        for j in range(self.nh):\r\n",
        "            for k in range(self.no):\r\n",
        "                self.wo[j][k] = rand(-2.0, 2.0)\r\n",
        "\r\n",
        "        # last change in weights for momentum   \r\n",
        "        self.ci = makeMatrix(self.ni, self.nh)\r\n",
        "        self.co = makeMatrix(self.nh, self.no)\r\n",
        "\r\n",
        "    def update(self, inputs):\r\n",
        "        if len(inputs) != self.ni-1:\r\n",
        "            raise ValueError('wrong number of inputs')\r\n",
        "\r\n",
        "        # input activations\r\n",
        "        for i in range(self.ni-1):\r\n",
        "            #self.ai[i] = sigmoid(inputs[i])\r\n",
        "            self.ai[i] = inputs[i]\r\n",
        "\r\n",
        "        # hidden activations\r\n",
        "        for j in range(self.nh):\r\n",
        "            sum = 0.0\r\n",
        "            for i in range(self.ni):\r\n",
        "                sum = sum + self.ai[i] * self.wi[i][j]\r\n",
        "            self.ah[j] = sigmoid(sum)\r\n",
        "\r\n",
        "        # output activations\r\n",
        "        for k in range(self.no):\r\n",
        "            sum = 0.0\r\n",
        "            for j in range(self.nh):\r\n",
        "                sum = sum + self.ah[j] * self.wo[j][k]\r\n",
        "            self.ao[k] = sigmoid(sum)\r\n",
        "\r\n",
        "        return self.ao[:]\r\n",
        "\r\n",
        "\r\n",
        "    def backPropagate(self, targets, N, M):\r\n",
        "        if len(targets) != self.no:\r\n",
        "            raise ValueError('wrong number of target values')\r\n",
        "\r\n",
        "        # calculate error terms for output\r\n",
        "        output_deltas = [0.0] * self.no\r\n",
        "        for k in range(self.no):\r\n",
        "            error = targets[k]-self.ao[k]\r\n",
        "            output_deltas[k] = dsigmoid(self.ao[k]) * error\r\n",
        "\r\n",
        "        # calculate error terms for hidden\r\n",
        "        hidden_deltas = [0.0] * self.nh\r\n",
        "        for j in range(self.nh):\r\n",
        "            error = 0.0\r\n",
        "            for k in range(self.no):\r\n",
        "                error = error + output_deltas[k]*self.wo[j][k]\r\n",
        "            hidden_deltas[j] = dsigmoid(self.ah[j]) * error\r\n",
        "\r\n",
        "        # update output weights\r\n",
        "        for j in range(self.nh):\r\n",
        "            for k in range(self.no):\r\n",
        "                change = output_deltas[k]*self.ah[j]\r\n",
        "                self.wo[j][k] = self.wo[j][k] + N*change + M*self.co[j][k]\r\n",
        "                self.co[j][k] = change\r\n",
        "                #print N*change, M*self.co[j][k]\r\n",
        "\r\n",
        "        # update input weights\r\n",
        "        for i in range(self.ni):\r\n",
        "            for j in range(self.nh):\r\n",
        "                change = hidden_deltas[j]*self.ai[i]\r\n",
        "                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]\r\n",
        "                self.ci[i][j] = change\r\n",
        "\r\n",
        "        # calculate error\r\n",
        "        error = 0.0\r\n",
        "        for k in range(len(targets)):\r\n",
        "            error = error + 0.5*(targets[k]-self.ao[k])**2\r\n",
        "        return error\r\n",
        "\r\n",
        "\r\n",
        "    def test(self, patterns):\r\n",
        "        for p in patterns:\r\n",
        "            print(p[0], '->', self.update(p[0]))\r\n",
        "\r\n",
        "    def weights(self):\r\n",
        "        print('Input weights:')\r\n",
        "        for i in range(self.ni):\r\n",
        "            print(self.wi[i])\r\n",
        "        print()\r\n",
        "        print('Output weights:')\r\n",
        "        for j in range(self.nh):\r\n",
        "            print(self.wo[j])\r\n",
        "\r\n",
        "    def train(self, patterns, iterations=1000, N=0.5, M=0.1):\r\n",
        "        # N: learning rate\r\n",
        "        # M: momentum factor\r\n",
        "        for i in range(iterations):\r\n",
        "            error = 0.0\r\n",
        "            for p in patterns:\r\n",
        "                inputs = p[0]\r\n",
        "                targets = p[1]\r\n",
        "                self.update(inputs)\r\n",
        "                error = error + self.backPropagate(targets, N, M)\r\n",
        "            if i % 100 == 0:\r\n",
        "                print('error %-.5f' % error)\r\n",
        "\r\n",
        "\r\n",
        "def demo():\r\n",
        "    # Teach network XOR function\r\n",
        "    pat = [\r\n",
        "        [[0,0], [0]],\r\n",
        "        [[0,1], [1]],\r\n",
        "        [[1,0], [1]],\r\n",
        "        [[1,1], [0]]\r\n",
        "    ]\r\n",
        "\r\n",
        "    # create a network with two input, two hidden, and one output nodes\r\n",
        "    n = NN(2, 2, 1)\r\n",
        "    # train it with some patterns\r\n",
        "    n.train(pat)\r\n",
        "    # test it\r\n",
        "    n.test(pat)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    demo()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error 0.94250\n",
            "error 0.04287\n",
            "error 0.00348\n",
            "error 0.00164\n",
            "error 0.00106\n",
            "error 0.00078\n",
            "error 0.00063\n",
            "error 0.00053\n",
            "error 0.00044\n",
            "error 0.00038\n",
            "[0, 0] -> [0.00424108155062589]\n",
            "[0, 1] -> [0.9821508029410748]\n",
            "[1, 0] -> [0.9820129388618121]\n",
            "[1, 1] -> [-0.0011469114721422528]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}